### `llama.cpp`

`LLama.cpp` is a library that allows you run a variety of models locally, on your machine. A variety of operating system are supported and there is usually a path to getting things working regardless of what CPU, GPU, or architecture you are running.

We will be focusing on `llama-cpp-python` but the core library is worth exploring too. You can find more information on the [GitHub repository for llama.cpp](https://github.com/ggerganov/llama.cpp).

#### Quantisation

##### What it is

Quantisation is a technique that reduces the precision of the numbers used to represent a model's parameters, such as weights and activations. This typically involves converting 32-bit floating-point numbers to lower-bit formats like 16-bit or 8-bit integers.

##### Why it matters

Quantisation is important because it can significantly reduce the memory footprint and computational requirements of machine learning models. This makes it possible to run complex models on resource-constrained devices, such as smartphones and embedded systems, without a substantial loss in accuracy.

##### How to find specific quants

Quantised versions of models are now readily available on the Hugging Face Hub. You can usually simply filter the results using the relevant library or tag. For LLama.cpp we are specifically interested in the GGUF format that is found in the ‘Libraries” tab. Many different quantisations are available, the Q4 quantisations typically perform well.

#### `llama-cpp-python`

`llama-cpp-python` offers a simple python interface for llama.cpp making integrating it into other python application very straightforward.

Installing can be complex but the installation instructions are relatively thorough, [find the relevant section for your hardware and follow the instructions](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#installation).

When we have everything installed usage is pretty simple. First we create an instance of the model:

```py
from llama_cpp import Llama

llm = Llama(
      model_path="./models/7B/llama-model.gguf",
      # n_gpu_layers=-1, # Uncomment to use GPU acceleration
      # seed=1337, # Uncomment to set a specific seed
      # n_ctx=2048, # Uncomment to increase the context window
)
```

Then we can use it:

```py
output = llm(
      "Q: Name the planets in the solar system? A: ", # Prompt
      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
      stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
      echo=True # Echo the prompt back in the output
)

print(output)
```

This library, like llama.cpp itself, is very powerful. I recommended [reading the docs](https://llama-cpp-python.readthedocs.io/en/stable/) for more specific information.